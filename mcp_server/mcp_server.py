#!/usr/bin/env python3
"""
BasicWebCrawler MCP Server

åŸºäºFastMCPçš„MCPæœåŠ¡å™¨ï¼Œæä¾›ç½‘é¡µçˆ¬è™«åŠŸèƒ½ï¼Œæ”¯æŒï¼š
- å•ä¸ªURLçˆ¬å–å¹¶è½¬æ¢ä¸ºMarkdown
- ä»æ–‡æœ¬ä¸­æå–URLå¹¶æ‰¹é‡çˆ¬å–
- å›¾ç‰‡ä¸‹è½½å’Œæœ¬åœ°åŒ–
- å¤šç§ç½‘ç«™ç‰¹å®šé…ç½®æ”¯æŒ
"""

import asyncio
import json
import os
import sys
import time
import tempfile
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥crawleræ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))

from fastmcp import FastMCP
from crawler import (
    fetch_and_convert_to_markdown,
    process_url_text_mode,
    extract_urls_from_text,
    get_site_config,
    sanitize_filename
)

# åˆ›å»ºFastMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("BasicWebCrawler")

@mcp.tool()
def crawl_single_url(
    url: str,
    img_folder: str = "images",
    use_cookies: bool = False,
    cookies_file: Optional[str] = None
) -> str:
    """
    çˆ¬å–å•ä¸ªç½‘é¡µå¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        url: è¦çˆ¬å–çš„ç½‘é¡µURL
        img_folder: å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º"images"
        use_cookies: æ˜¯å¦ä½¿ç”¨cookiesæ–‡ä»¶
        cookies_file: cookiesæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
        
    Returns:
        åŒ…å«çˆ¬å–ç»“æœä¿¡æ¯çš„å­—ç¬¦ä¸²
    """
    try:
        # å¤„ç†cookies
        cookies = None
        if use_cookies and cookies_file and os.path.exists(cookies_file):
            try:
                with open(cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
            except Exception as e:
                return f"è¯»å–cookiesæ–‡ä»¶å¤±è´¥: {str(e)}"
        
        # æ£€æŸ¥ç½‘ç«™é…ç½®
        site_config = get_site_config(url)
        if site_config['needs_cookies'] and not cookies:
            return f"è­¦å‘Š: ç½‘ç«™ {urlparse(url).netloc} å¯èƒ½éœ€è¦cookiesæ‰èƒ½æ­£å¸¸è®¿é—®ã€‚å»ºè®®ä½¿ç”¨cookieså‚æ•°ã€‚"
        
        # å¼€å§‹çˆ¬å–
        start_time = time.time()
        markdown_output, page_title = fetch_and_convert_to_markdown(url, img_folder, cookies)
        end_time = time.time()
        
        if markdown_output:
            # ç”Ÿæˆæ–‡ä»¶å
            sanitized_title = sanitize_filename(page_title)
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = f"{sanitized_title}_{timestamp}.md"
            
            # ä¿å­˜æ–‡ä»¶åˆ°é¡¹ç›®æ ¹ç›®å½•
            try:
                # è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
                project_root = Path(__file__).parent.parent
                output_path = project_root / output_file
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_output)
                
                return f"âœ… çˆ¬å–æˆåŠŸï¼\n\n" \
                       f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}\n" \
                       f"ğŸ”— URL: {url}\n" \
                       f"ğŸ“ ä¿å­˜æ–‡ä»¶: {output_file}\n" \
                       f"ğŸ–¼ï¸ å›¾ç‰‡ç›®å½•: {img_folder}/\n" \
                       f"â±ï¸ è€—æ—¶: {end_time - start_time:.2f} ç§’\n\n" \
                       f"å†…å®¹é¢„è§ˆ:\n{markdown_output[:500]}..."
                       
            except OSError as e:
                # ä½¿ç”¨å¤‡ç”¨æ–‡ä»¶å
                fallback_filename = f"webpage_{timestamp}.md"
                project_root = Path(__file__).parent.parent
                fallback_path = project_root / fallback_filename
                with open(fallback_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_output)
                return f"âœ… çˆ¬å–æˆåŠŸï¼ˆä½¿ç”¨å¤‡ç”¨æ–‡ä»¶åï¼‰ï¼\næ–‡ä»¶: {fallback_filename}\nè€—æ—¶: {end_time - start_time:.2f} ç§’"
        else:
            return f"âŒ çˆ¬å–å¤±è´¥: æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®æˆ–ç½‘ç«™æ˜¯å¦å¯è®¿é—®"
            
    except Exception as e:
        return f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def crawl_urls_from_text(
    text: str,
    img_folder: str = "images",
    use_cookies: bool = False,
    cookies_file: Optional[str] = None
) -> str:
    """
    ä»æ–‡æœ¬ä¸­æå–URLå¹¶æ‰¹é‡çˆ¬å–
    
    Args:
        text: åŒ…å«URLçš„æ–‡æœ¬å†…å®¹
        img_folder: å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º"images"
        use_cookies: æ˜¯å¦ä½¿ç”¨cookiesæ–‡ä»¶
        cookies_file: cookiesæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
        
    Returns:
        åŒ…å«æ‰¹é‡çˆ¬å–ç»“æœçš„å­—ç¬¦ä¸²
    """
    try:
        # å¤„ç†cookies
        cookies = None
        if use_cookies and cookies_file and os.path.exists(cookies_file):
            try:
                with open(cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
            except Exception as e:
                return f"è¯»å–cookiesæ–‡ä»¶å¤±è´¥: {str(e)}"
        
        # æå–URL
        urls = extract_urls_from_text(text)
        if not urls:
            return "âŒ æœªåœ¨æä¾›çš„æ–‡æœ¬ä¸­æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„URL"
        
        # å¼€å§‹æ‰¹é‡çˆ¬å–
        start_time = time.time()
        markdown_output, result_summary = process_url_text_mode(text, img_folder, cookies)
        end_time = time.time()
        
        if markdown_output:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = f"æ‰¹é‡çˆ¬å–ç»“æœ_{timestamp}.md"
            
            # ä¿å­˜æ–‡ä»¶åˆ°é¡¹ç›®æ ¹ç›®å½•
            try:
                # è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
                project_root = Path(__file__).parent.parent
                output_path = project_root / output_file
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_output)
                
                return f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆï¼\n\n" \
                       f"ğŸ“Š {result_summary}\n" \
                       f"ğŸ“ ä¿å­˜æ–‡ä»¶: {output_file}\n" \
                       f"ğŸ–¼ï¸ å›¾ç‰‡ç›®å½•: {img_folder}/\n" \
                       f"â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’\n\n" \
                       f"å‘ç°çš„URL:\n" + "\n".join([f"â€¢ {url}" for url in urls])
                       
            except OSError as e:
                fallback_filename = f"æ‰¹é‡çˆ¬å–_{timestamp}.md"
                project_root = Path(__file__).parent.parent
                fallback_path = project_root / fallback_filename
                with open(fallback_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_output)
                return f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆï¼ˆä½¿ç”¨å¤‡ç”¨æ–‡ä»¶åï¼‰ï¼\næ–‡ä»¶: {fallback_filename}"
        else:
            return f"âŒ æ‰¹é‡çˆ¬å–å¤±è´¥: æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•URL"
            
    except Exception as e:
        return f"âŒ æ‰¹é‡çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def extract_urls(text: str) -> str:
    """
    ä»æ–‡æœ¬ä¸­æå–URLåˆ—è¡¨
    
    Args:
        text: è¦åˆ†æçš„æ–‡æœ¬å†…å®¹
        
    Returns:
        æå–åˆ°çš„URLåˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    try:
        urls = extract_urls_from_text(text)
        
        if not urls:
            return "âŒ æœªåœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„URL"
        
        result = f"âœ… æˆåŠŸæå–åˆ° {len(urls)} ä¸ªURL:\n\n"
        for i, url in enumerate(urls, 1):
            # è·å–ç½‘ç«™é…ç½®ä¿¡æ¯
            site_config = get_site_config(url)
            domain = urlparse(url).netloc
            needs_cookies = "ğŸ” éœ€è¦cookies" if site_config['needs_cookies'] else "ğŸ”“ æ— éœ€cookies"
            
            result += f"{i}. {url}\n"
            result += f"   åŸŸå: {domain} | {needs_cookies}\n\n"
        
        return result
        
    except Exception as e:
        return f"âŒ URLæå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def check_site_config(url: str) -> str:
    """
    æ£€æŸ¥ç½‘ç«™çš„ç‰¹å®šé…ç½®ä¿¡æ¯
    
    Args:
        url: è¦æ£€æŸ¥çš„ç½‘ç«™URL
        
    Returns:
        ç½‘ç«™é…ç½®ä¿¡æ¯
    """
    try:
        site_config = get_site_config(url)
        domain = urlparse(url).netloc
        
        result = f"ğŸŒ ç½‘ç«™é…ç½®ä¿¡æ¯ - {domain}\n\n"
        result += f"ğŸ” éœ€è¦cookies: {'æ˜¯' if site_config['needs_cookies'] else 'å¦'}\n"
        result += f"ğŸ¯ å†…å®¹é€‰æ‹©å™¨: {', '.join(site_config['main_content_selectors'])}\n"
        result += f"ğŸ“‹ è¯·æ±‚å¤´æ•°é‡: {len(site_config['headers'])}\n\n"
        
        result += "ğŸ“‹ è¯·æ±‚å¤´è¯¦æƒ…:\n"
        for key, value in site_config['headers'].items():
            result += f"  â€¢ {key}: {value}\n"
        
        return result
        
    except Exception as e:
        return f"âŒ æ£€æŸ¥ç½‘ç«™é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.tool()
def get_supported_sites() -> str:
    """
    è·å–æ”¯æŒçš„ç½‘ç«™åˆ—è¡¨å’Œé…ç½®ä¿¡æ¯
    
    Returns:
        æ”¯æŒçš„ç½‘ç«™åˆ—è¡¨
    """
    try:
        from crawler import SITE_CONFIGS
        
        result = "ğŸŒ æ”¯æŒçš„ç½‘ç«™é…ç½®:\n\n"
        
        for site, config in SITE_CONFIGS.items():
            if site == 'default':
                continue
                
            result += f"ğŸ”— {site}\n"
            result += f"   ğŸ” éœ€è¦cookies: {'æ˜¯' if config['needs_cookies'] else 'å¦'}\n"
            result += f"   ğŸ¯ å†…å®¹é€‰æ‹©å™¨: {len(config['main_content_selectors'])} ä¸ª\n"
            result += f"   ğŸ“‹ è‡ªå®šä¹‰è¯·æ±‚å¤´: {len(config['headers'])} ä¸ª\n\n"
        
        result += "ğŸ“ è¯´æ˜:\n"
        result += "â€¢ æ”¯æŒçš„ç½‘ç«™æœ‰ä¸“é—¨ä¼˜åŒ–çš„å†…å®¹æå–é…ç½®\n"
        result += "â€¢ å…¶ä»–ç½‘ç«™å°†ä½¿ç”¨é€šç”¨é…ç½®è¿›è¡Œçˆ¬å–\n"
        result += "â€¢ éœ€è¦cookiesçš„ç½‘ç«™å»ºè®®æä¾›cookiesæ–‡ä»¶ä»¥è·å¾—æ›´å¥½çš„çˆ¬å–æ•ˆæœ\n"
        
        return result
        
    except Exception as e:
        return f"âŒ è·å–ç½‘ç«™åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.resource("crawler://config")
def get_crawler_config():
    """
    æä¾›çˆ¬è™«çš„é…ç½®ä¿¡æ¯
    """
    try:
        from crawler import SITE_CONFIGS, IGNORED_EXTENSIONS
        
        config_info = {
            "supported_sites": list(SITE_CONFIGS.keys()),
            "ignored_image_extensions": IGNORED_EXTENSIONS,
            "default_image_folder": "images",
            "features": [
                "å•ä¸ªURLçˆ¬å–",
                "æ‰¹é‡URLæå–å’Œçˆ¬å–",
                "å›¾ç‰‡ä¸‹è½½å’Œæœ¬åœ°åŒ–",
                "å¤šç½‘ç«™ç‰¹å®šé…ç½®",
                "Cookiesæ”¯æŒ",
                "Markdownæ ¼å¼è¾“å‡º"
            ]
        }
        
        return json.dumps(config_info, ensure_ascii=False, indent=2)
        
    except Exception as e:
        return f"è·å–é…ç½®ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

@mcp.prompt()
def crawl_webpage_prompt(url: str, use_cookies: bool = False):
    """
    çˆ¬å–ç½‘é¡µçš„æç¤ºæ¨¡æ¿
    
    Args:
        url: è¦çˆ¬å–çš„ç½‘é¡µURL
        use_cookies: æ˜¯å¦ä½¿ç”¨cookies
    """
    prompt_text = f"""æˆ‘éœ€è¦çˆ¬å–è¿™ä¸ªç½‘é¡µçš„å†…å®¹: {url}

è¯·å¸®æˆ‘ï¼š
1. çˆ¬å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
2. ä¸‹è½½å¹¶æœ¬åœ°åŒ–æ‰€æœ‰å›¾ç‰‡
3. æä¾›çˆ¬å–ç»“æœçš„è¯¦ç»†ä¿¡æ¯

{'æ³¨æ„ï¼šå¦‚æœè¿™ä¸ªç½‘ç«™éœ€è¦ç™»å½•ï¼Œè¯·ç¡®ä¿æä¾›æœ‰æ•ˆçš„cookiesæ–‡ä»¶ã€‚' if use_cookies else ''}

è¯·ä½¿ç”¨ crawl_single_url å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚"""

    return prompt_text

@mcp.prompt()
def batch_crawl_prompt(text_content: str):
    """
    æ‰¹é‡çˆ¬å–çš„æç¤ºæ¨¡æ¿
    
    Args:
        text_content: åŒ…å«URLçš„æ–‡æœ¬å†…å®¹
    """
    prompt_text = f"""æˆ‘æœ‰ä¸€æ®µåŒ…å«å¤šä¸ªURLçš„æ–‡æœ¬ï¼Œéœ€è¦æ‰¹é‡çˆ¬å–è¿™äº›ç½‘é¡µï¼š

æ–‡æœ¬å†…å®¹ï¼š
{text_content}

è¯·å¸®æˆ‘ï¼š
1. ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æœ‰æ•ˆçš„URL
2. æ‰¹é‡çˆ¬å–æ¯ä¸ªURLçš„å†…å®¹
3. å°†æ‰€æœ‰å†…å®¹åˆå¹¶åˆ°ä¸€ä¸ªMarkdownæ–‡ä»¶ä¸­
4. æä¾›è¯¦ç»†çš„çˆ¬å–ç»Ÿè®¡ä¿¡æ¯

è¯·ä½¿ç”¨ crawl_urls_from_text å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚"""

    return prompt_text

def main():
    """è¿è¡ŒMCPæœåŠ¡å™¨"""
    mcp.run()

if __name__ == "__main__":
    main() 